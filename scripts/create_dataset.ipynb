{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03021ca2",
   "metadata": {},
   "source": [
    "# Dataset Creation for Health Sensing Breathing Analysis 📊\n",
    "\n",
    "## Machine Learning Dataset Generation\n",
    "\n",
    "### Task Overview\n",
    "Create a labeled dataset from 8-hour sleep study recordings by splitting continuous signals into 30-second windows with 50% overlap. Each window will be labeled based on breathing events (Hypopnea, Obstructive Apnea, or Normal).\n",
    "\n",
    "### Key Requirements ✅\n",
    "- **Window Size**: 30-second segments with 50% overlap (15-second step)\n",
    "- **Label Assignment**: Based on >50% overlap with breathing events\n",
    "- **Target Labels**: Hypopnea, Obstructive Apnea, Normal\n",
    "- **Input Signals**: Nasal Airflow, Thoracic Movement, SpO₂\n",
    "- **Output Format**: Efficient storage for ML training\n",
    "\n",
    "### Technical Approach\n",
    "- **Sliding Window**: Extract overlapping time segments from continuous signals\n",
    "- **Event Mapping**: Assign labels based on temporal overlap with annotations\n",
    "- **Data Format**: Parquet format for efficient storage and fast loading\n",
    "- **Feature Engineering**: Time-series windows ready for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fadbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For data storage\n",
    "import pickle\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(\"📊 Ready for dataset creation from sleep study data\")\n",
    "print(\"🎯 Target: 30-second windows with event-based labeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCreator:\n",
    "    \"\"\"\n",
    "    Creates labeled dataset from continuous sleep study recordings.\n",
    "    Splits 8-hour signals into 30-second windows with 50% overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_duration=30, overlap_ratio=0.5, sampling_rate=32):\n",
    "        \"\"\"\n",
    "        Initialize the dataset creator.\n",
    "        \n",
    "        Args:\n",
    "            window_duration (int): Window size in seconds (default: 30)\n",
    "            overlap_ratio (float): Overlap ratio between windows (default: 0.5 for 50%)\n",
    "            sampling_rate (int): Sampling rate in Hz (default: 32)\n",
    "        \"\"\"\n",
    "        self.window_duration = window_duration\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.sampling_rate = sampling_rate\n",
    "        \n",
    "        # Calculate window parameters\n",
    "        self.window_samples = window_duration * sampling_rate  # 30 * 32 = 960 samples\n",
    "        self.step_samples = int(window_samples * (1 - overlap_ratio))  # 480 samples (15 seconds)\n",
    "        \n",
    "        # Target labels for the dataset\n",
    "        self.target_labels = ['Hypopnea', 'Obstructive Apnea', 'Normal']\n",
    "        \n",
    "        print(f\"✅ DatasetCreator initialized\")\n",
    "        print(f\"   ⏱️  Window duration: {window_duration} seconds ({self.window_samples} samples)\")\n",
    "        print(f\"   🔄 Overlap: {overlap_ratio*100}% ({window_duration * overlap_ratio} seconds)\")\n",
    "        print(f\"   👣 Step size: {self.step_samples} samples ({self.step_samples/sampling_rate} seconds)\")\n",
    "        print(f\"   🏷️  Target labels: {self.target_labels}\")\n",
    "    \n",
    "    def parse_datetime(self, date_str):\n",
    "        \"\"\"Parse datetime strings from various formats found in the data files.\"\"\"\n",
    "        formats = [\n",
    "            \"%d.%m.%Y %H:%M:%S,%f\",  # 30.05.2024 20:59:00,000\n",
    "            \"%d.%m.%Y %H:%M:%S\",     # 30.05.2024 20:59:00\n",
    "            \"%d-%m-%Y %H:%M:%S,%f\",  # 30-05-2024 21:22:45,000\n",
    "            \"%d-%m-%Y %H:%M:%S\",     # 30-05-2024 21:22:45\n",
    "            \"%d_%m_%Y %H:%M:%S,%f\",  # Alternative format\n",
    "            \"%m/%d/%Y %I:%M:%S %p\",  # 5/30/2024 8:59:00 PM\n",
    "        ]\n",
    "        \n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                return datetime.strptime(date_str.strip(), fmt)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        # Manual parsing for edge cases\n",
    "        try:\n",
    "            clean_str = date_str.replace(';', '').strip()\n",
    "            if ',' in clean_str:\n",
    "                dt_part, ms_part = clean_str.rsplit(',', 1)\n",
    "                for base_fmt in [\"%d.%m.%Y %H:%M:%S\", \"%d-%m-%Y %H:%M:%S\"]:\n",
    "                    try:\n",
    "                        dt = datetime.strptime(dt_part, base_fmt)\n",
    "                        ms = int(ms_part)\n",
    "                        return dt + timedelta(milliseconds=ms)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            else:\n",
    "                for base_fmt in [\"%d.%m.%Y %H:%M:%S\", \"%d-%m-%Y %H:%M:%S\"]:\n",
    "                    try:\n",
    "                        return datetime.strptime(clean_str, base_fmt)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def find_file_by_pattern(self, folder, patterns):\n",
    "        \"\"\"Find files matching any of the given patterns.\"\"\"\n",
    "        for pattern in patterns:\n",
    "            files = glob.glob(os.path.join(folder, pattern))\n",
    "            if files:\n",
    "                return files[0]\n",
    "        return None\n",
    "\n",
    "print(\"✅ DatasetCreator class defined!\")\n",
    "print(\"🔧 Ready to process sleep study signals into ML-ready windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6cc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def load_signal_data(self, file_path, signal_type):\n",
    "        \"\"\"Load signal data with timestamps from a file.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"⚠️  Warning: Signal file not found: {os.path.basename(file_path)}\")\n",
    "            return None\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Extract start time from header\n",
    "        start_time_line = None\n",
    "        for line in lines[:4]:\n",
    "            if 'Start Time:' in line:\n",
    "                start_time_line = line\n",
    "                break\n",
    "        \n",
    "        if not start_time_line:\n",
    "            print(f\"⚠️  Warning: No start time found in {os.path.basename(file_path)}\")\n",
    "            return None\n",
    "        \n",
    "        start_time_str = start_time_line.split('Start Time:')[1].strip()\n",
    "        start_time = self.parse_datetime(start_time_str)\n",
    "        \n",
    "        if not start_time:\n",
    "            print(f\"⚠️  Warning: Could not parse start time: {start_time_str}\")\n",
    "            return None\n",
    "        \n",
    "        # Parse data lines\n",
    "        values = []\n",
    "        timestamps = []\n",
    "        \n",
    "        for line in lines[4:]:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                if ';' in line:\n",
    "                    time_str, value_str = line.split(';', 1)\n",
    "                else:\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 2:\n",
    "                        time_str, value_str = parts[0], parts[1]\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                timestamp = self.parse_datetime(time_str.strip())\n",
    "                if timestamp:\n",
    "                    timestamps.append(timestamp)\n",
    "                    values.append(float(value_str.strip()))\n",
    "                    \n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "        \n",
    "        if not timestamps:\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'timestamp': timestamps,\n",
    "            'value': values,\n",
    "            'signal_type': signal_type\n",
    "        })\n",
    "        \n",
    "        df = df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')\n",
    "        df = df.set_index('timestamp')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def load_events_data(self, file_path):\n",
    "        \"\"\"Load breathing events data.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            return None\n",
    "            \n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        events = []\n",
    "        data_lines = [line.strip() for line in lines[4:] if line.strip()]\n",
    "        \n",
    "        for line in data_lines:\n",
    "            try:\n",
    "                main_parts = line.split(';')\n",
    "                if len(main_parts) < 3:\n",
    "                    continue\n",
    "                    \n",
    "                time_range = main_parts[0].strip()\n",
    "                duration = float(main_parts[1].strip())\n",
    "                event_type = main_parts[2].strip()\n",
    "                sleep_stage = main_parts[3].strip() if len(main_parts) > 3 else \"\"\n",
    "                \n",
    "                # Parse time range\n",
    "                dash_pos = time_range.rfind('-')\n",
    "                if dash_pos < 0:\n",
    "                    continue\n",
    "                    \n",
    "                start_str = time_range[:dash_pos].strip()\n",
    "                end_time_part = time_range[dash_pos+1:].strip()\n",
    "                \n",
    "                date_part = start_str.split(' ')[0]\n",
    "                end_str = f\"{date_part} {end_time_part}\"\n",
    "                \n",
    "                start_time = self.parse_datetime(start_str)\n",
    "                end_time = self.parse_datetime(end_str)\n",
    "                \n",
    "                if start_time and end_time:\n",
    "                    events.append({\n",
    "                        'start': start_time,\n",
    "                        'end': end_time,\n",
    "                        'duration': duration,\n",
    "                        'event_type': event_type,\n",
    "                        'sleep_stage': sleep_stage\n",
    "                    })\n",
    "            except Exception:\n",
    "                continue\n",
    "                \n",
    "        if events:\n",
    "            return pd.DataFrame(events)\n",
    "        return None\n",
    "\n",
    "# Add these methods to the DatasetCreator class\n",
    "DatasetCreator.load_signal_data = load_signal_data\n",
    "DatasetCreator.load_events_data = load_events_data\n",
    "\n",
    "print(\"✅ Signal loading methods added!\")\n",
    "print(\"📥 Ready to load signals and events from participant folders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e84cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def extract_windows(self, signal_data, signal_name):\n",
    "        \"\"\"\n",
    "        Extract overlapping 30-second windows from signal data.\n",
    "        \n",
    "        Args:\n",
    "            signal_data (DataFrame): Signal data with timestamp index\n",
    "            signal_name (str): Name of the signal\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples: (window_start_time, window_end_time, window_values)\n",
    "        \"\"\"\n",
    "        if signal_data is None or len(signal_data) < self.window_samples:\n",
    "            return []\n",
    "        \n",
    "        windows = []\n",
    "        values = signal_data['value'].values\n",
    "        timestamps = signal_data.index\n",
    "        \n",
    "        # Extract overlapping windows\n",
    "        for start_idx in range(0, len(values) - self.window_samples + 1, self.step_samples):\n",
    "            end_idx = start_idx + self.window_samples\n",
    "            \n",
    "            window_values = values[start_idx:end_idx]\n",
    "            window_start_time = timestamps[start_idx]\n",
    "            window_end_time = timestamps[end_idx - 1]\n",
    "            \n",
    "            windows.append((window_start_time, window_end_time, window_values))\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def assign_label(self, window_start, window_end, events_data):\n",
    "        \"\"\"\n",
    "        Assign label to a window based on overlap with breathing events.\n",
    "        \n",
    "        Args:\n",
    "            window_start (datetime): Window start time\n",
    "            window_end (datetime): Window end time\n",
    "            events_data (DataFrame): Events data\n",
    "            \n",
    "        Returns:\n",
    "            str: Label for the window ('Hypopnea', 'Obstructive Apnea', or 'Normal')\n",
    "        \"\"\"\n",
    "        if events_data is None or len(events_data) == 0:\n",
    "            return 'Normal'\n",
    "        \n",
    "        window_duration = (window_end - window_start).total_seconds()\n",
    "        \n",
    "        # Check overlap with each event\n",
    "        for _, event in events_data.iterrows():\n",
    "            event_type = event['event_type']\n",
    "            \n",
    "            # Only consider target labels\n",
    "            if event_type not in ['Hypopnea', 'Obstructive Apnea']:\n",
    "                continue\n",
    "            \n",
    "            event_start = event['start']\n",
    "            event_end = event['end']\n",
    "            \n",
    "            # Calculate overlap\n",
    "            overlap_start = max(window_start, event_start)\n",
    "            overlap_end = min(window_end, event_end)\n",
    "            \n",
    "            if overlap_start < overlap_end:\n",
    "                overlap_duration = (overlap_end - overlap_start).total_seconds()\n",
    "                overlap_ratio = overlap_duration / window_duration\n",
    "                \n",
    "                # If more than 50% overlap, assign the event label\n",
    "                if overlap_ratio > 0.5:\n",
    "                    return event_type\n",
    "        \n",
    "        return 'Normal'\n",
    "    \n",
    "    def process_participant(self, participant_folder):\n",
    "        \"\"\"\n",
    "        Process a single participant to extract labeled windows.\n",
    "        \n",
    "        Args:\n",
    "            participant_folder (str): Path to participant folder\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries: Windows with features and labels\n",
    "        \"\"\"\n",
    "        participant_id = os.path.basename(participant_folder)\n",
    "        print(f\"🔄 Processing {participant_id}...\")\n",
    "        \n",
    "        # Define file patterns for different naming conventions\n",
    "        flow_patterns = [\n",
    "            \"Flow - *.txt\", \"Flow  - *.txt\", \"Flow Signal - *.txt\", \"Flow Nasal - *.txt\"\n",
    "        ]\n",
    "        thorac_patterns = [\n",
    "            \"Thorac - *.txt\", \"Thorac  - *.txt\", \"Thorac Signal - *.txt\", \"Thorac Movement - *.txt\"\n",
    "        ]\n",
    "        spo2_patterns = [\n",
    "            \"SPO2 - *.txt\", \"SPO2  - *.txt\", \"SPO2 Signal - *.txt\"\n",
    "        ]\n",
    "        events_patterns = [\n",
    "            \"Flow Events - *.txt\", \"Flow Events  - *.txt\"\n",
    "        ]\n",
    "        \n",
    "        # Load signals\n",
    "        signals = {}\n",
    "        \n",
    "        flow_file = self.find_file_by_pattern(participant_folder, flow_patterns)\n",
    "        if flow_file:\n",
    "            signals['nasal_airflow'] = self.load_signal_data(flow_file, 'Nasal Airflow')\n",
    "        \n",
    "        thorac_file = self.find_file_by_pattern(participant_folder, thorac_patterns)\n",
    "        if thorac_file:\n",
    "            signals['thoracic_movement'] = self.load_signal_data(thorac_file, 'Thoracic Movement')\n",
    "        \n",
    "        spo2_file = self.find_file_by_pattern(participant_folder, spo2_patterns)\n",
    "        if spo2_file:\n",
    "            signals['spo2'] = self.load_signal_data(spo2_file, 'SpO₂')\n",
    "        \n",
    "        # Load events\n",
    "        events_file = self.find_file_by_pattern(participant_folder, events_patterns)\n",
    "        events_data = self.load_events_data(events_file) if events_file else None\n",
    "        \n",
    "        # Filter events to only target labels\n",
    "        if events_data is not None:\n",
    "            target_events = events_data[events_data['event_type'].isin(['Hypopnea', 'Obstructive Apnea'])]\n",
    "        else:\n",
    "            target_events = None\n",
    "        \n",
    "        print(f\"   📊 Loaded signals: {list(signals.keys())}\")\n",
    "        if target_events is not None:\n",
    "            event_counts = target_events['event_type'].value_counts()\n",
    "            print(f\"   🚨 Events: {dict(event_counts)}\")\n",
    "        else:\n",
    "            print(f\"   🚨 Events: None found\")\n",
    "        \n",
    "        # Extract windows from the primary signal (nasal airflow)\n",
    "        if 'nasal_airflow' not in signals or signals['nasal_airflow'] is None:\n",
    "            print(f\"   ❌ No nasal airflow data found for {participant_id}\")\n",
    "            return []\n",
    "        \n",
    "        primary_signal = signals['nasal_airflow']\n",
    "        windows = self.extract_windows(primary_signal, 'nasal_airflow')\n",
    "        \n",
    "        print(f\"   🪟 Extracted {len(windows)} windows\")\n",
    "        \n",
    "        # Process each window\n",
    "        dataset_windows = []\n",
    "        \n",
    "        for i, (window_start, window_end, nasal_values) in enumerate(windows):\n",
    "            # Get corresponding values from other signals\n",
    "            thorac_values = self._get_window_values(signals.get('thoracic_movement'), \n",
    "                                                   window_start, window_end)\n",
    "            spo2_values = self._get_window_values(signals.get('spo2'), \n",
    "                                                 window_start, window_end)\n",
    "            \n",
    "            # Assign label\n",
    "            label = self.assign_label(window_start, window_end, target_events)\n",
    "            \n",
    "            # Create window data\n",
    "            window_data = {\n",
    "                'participant_id': participant_id,\n",
    "                'window_id': f\"{participant_id}_W{i:04d}\",\n",
    "                'start_time': window_start,\n",
    "                'end_time': window_end,\n",
    "                'duration': self.window_duration,\n",
    "                'label': label,\n",
    "                'nasal_airflow': nasal_values,\n",
    "                'thoracic_movement': thorac_values,\n",
    "                'spo2': spo2_values\n",
    "            }\n",
    "            \n",
    "            dataset_windows.append(window_data)\n",
    "        \n",
    "        # Label distribution\n",
    "        labels = [w['label'] for w in dataset_windows]\n",
    "        label_counts = pd.Series(labels).value_counts()\n",
    "        print(f\"   🏷️  Label distribution: {dict(label_counts)}\")\n",
    "        \n",
    "        return dataset_windows\n",
    "    \n",
    "    def _get_window_values(self, signal_data, window_start, window_end):\n",
    "        \"\"\"Get signal values for a specific time window.\"\"\"\n",
    "        if signal_data is None:\n",
    "            return None\n",
    "        \n",
    "        # Filter signal data for the window time range\n",
    "        window_data = signal_data[(signal_data.index >= window_start) & \n",
    "                                 (signal_data.index <= window_end)]\n",
    "        \n",
    "        if len(window_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        return window_data['value'].values\n",
    "\n",
    "# Add methods to the DatasetCreator class\n",
    "DatasetCreator.extract_windows = extract_windows\n",
    "DatasetCreator.assign_label = assign_label\n",
    "DatasetCreator.process_participant = process_participant\n",
    "DatasetCreator._get_window_values = _get_window_values\n",
    "\n",
    "print(\"✅ Window extraction and labeling methods added!\")\n",
    "print(\"🪟 Ready to create 30-second windows with event-based labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def create_dataset(self, input_dir, output_dir):\n",
    "        \"\"\"\n",
    "        Create the complete dataset from all participants.\n",
    "        \n",
    "        Args:\n",
    "            input_dir (str): Input directory containing participant folders\n",
    "            output_dir (str): Output directory for saving the dataset\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dataset creation summary\n",
    "        \"\"\"\n",
    "        print(\"🚀 DATASET CREATION: Processing all participants\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Find all participant folders\n",
    "        participants = [f for f in os.listdir(input_dir) \n",
    "                       if os.path.isdir(os.path.join(input_dir, f)) and f.startswith('AP')]\n",
    "        participants.sort()\n",
    "        \n",
    "        print(f\"📁 Found {len(participants)} participants: {participants}\")\n",
    "        \n",
    "        all_windows = []\n",
    "        participant_stats = {}\n",
    "        \n",
    "        # Process each participant\n",
    "        for i, participant in enumerate(participants, 1):\n",
    "            print(f\"\\n[{i}/{len(participants)}] Processing {participant}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            participant_path = os.path.join(input_dir, participant)\n",
    "            \n",
    "            try:\n",
    "                windows = self.process_participant(participant_path)\n",
    "                \n",
    "                if windows:\n",
    "                    all_windows.extend(windows)\n",
    "                    \n",
    "                    # Calculate statistics\n",
    "                    labels = [w['label'] for w in windows]\n",
    "                    participant_stats[participant] = {\n",
    "                        'total_windows': len(windows),\n",
    "                        'label_distribution': pd.Series(labels).value_counts().to_dict(),\n",
    "                        'duration_hours': len(windows) * self.window_duration / 3600,\n",
    "                        'status': 'success'\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"   ✅ {participant}: {len(windows)} windows created\")\n",
    "                else:\n",
    "                    participant_stats[participant] = {\n",
    "                        'total_windows': 0,\n",
    "                        'label_distribution': {},\n",
    "                        'duration_hours': 0,\n",
    "                        'status': 'failed'\n",
    "                    }\n",
    "                    print(f\"   ❌ {participant}: No windows created\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                participant_stats[participant] = {\n",
    "                    'total_windows': 0,\n",
    "                    'label_distribution': {},\n",
    "                    'duration_hours': 0,\n",
    "                    'status': 'error',\n",
    "                    'error': str(e)\n",
    "                }\n",
    "                print(f\"   ❌ {participant}: Error - {str(e)}\")\n",
    "        \n",
    "        # Dataset summary\n",
    "        print(f\"\\n\" + \"=\" * 70)\n",
    "        print(\"📋 DATASET CREATION SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        total_windows = len(all_windows)\n",
    "        if total_windows > 0:\n",
    "            # Overall label distribution\n",
    "            all_labels = [w['label'] for w in all_windows]\n",
    "            overall_distribution = pd.Series(all_labels).value_counts()\n",
    "            \n",
    "            print(f\"✅ Total windows created: {total_windows:,}\")\n",
    "            print(f\"📊 Overall label distribution:\")\n",
    "            for label, count in overall_distribution.items():\n",
    "                percentage = (count / total_windows) * 100\n",
    "                print(f\"   • {label}: {count:,} ({percentage:.1f}%)\")\n",
    "            \n",
    "            # Save dataset\n",
    "            print(f\"\\n💾 Saving dataset to: {output_dir}/\")\n",
    "            self.save_dataset(all_windows, output_dir)\n",
    "            \n",
    "            # Save statistics\n",
    "            stats_summary = {\n",
    "                'creation_date': datetime.now().isoformat(),\n",
    "                'total_windows': total_windows,\n",
    "                'participants': len(participants),\n",
    "                'window_duration_seconds': self.window_duration,\n",
    "                'overlap_ratio': self.overlap_ratio,\n",
    "                'overall_distribution': overall_distribution.to_dict(),\n",
    "                'participant_stats': participant_stats\n",
    "            }\n",
    "            \n",
    "            stats_path = os.path.join(output_dir, 'dataset_stats.json')\n",
    "            import json\n",
    "            with open(stats_path, 'w') as f:\n",
    "                # Convert datetime objects to strings for JSON serialization\n",
    "                def json_serializer(obj):\n",
    "                    if isinstance(obj, datetime):\n",
    "                        return obj.isoformat()\n",
    "                    return obj\n",
    "                \n",
    "                json.dump(stats_summary, f, indent=2, default=json_serializer)\n",
    "            \n",
    "            print(f\"📈 Statistics saved to: {stats_path}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ No windows were created from any participant\")\n",
    "            stats_summary = {}\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        return stats_summary\n",
    "    \n",
    "    def save_dataset(self, windows, output_dir):\n",
    "        \"\"\"\n",
    "        Save the dataset in multiple formats for different use cases.\n",
    "        \n",
    "        Args:\n",
    "            windows (list): List of window dictionaries\n",
    "            output_dir (str): Output directory\n",
    "        \"\"\"\n",
    "        print(\"💾 Saving dataset in multiple formats...\")\n",
    "        \n",
    "        # Prepare data for saving\n",
    "        dataset_records = []\n",
    "        \n",
    "        for window in windows:\n",
    "            # Create a flattened record for tabular formats\n",
    "            record = {\n",
    "                'participant_id': window['participant_id'],\n",
    "                'window_id': window['window_id'],\n",
    "                'start_time': window['start_time'],\n",
    "                'end_time': window['end_time'],\n",
    "                'duration': window['duration'],\n",
    "                'label': window['label']\n",
    "            }\n",
    "            \n",
    "            # Add signal features (basic statistics for now)\n",
    "            for signal_name in ['nasal_airflow', 'thoracic_movement', 'spo2']:\n",
    "                values = window[signal_name]\n",
    "                if values is not None and len(values) > 0:\n",
    "                    record[f'{signal_name}_mean'] = np.mean(values)\n",
    "                    record[f'{signal_name}_std'] = np.std(values)\n",
    "                    record[f'{signal_name}_min'] = np.min(values)\n",
    "                    record[f'{signal_name}_max'] = np.max(values)\n",
    "                    record[f'{signal_name}_samples'] = len(values)\n",
    "                else:\n",
    "                    record[f'{signal_name}_mean'] = None\n",
    "                    record[f'{signal_name}_std'] = None\n",
    "                    record[f'{signal_name}_min'] = None\n",
    "                    record[f'{signal_name}_max'] = None\n",
    "                    record[f'{signal_name}_samples'] = 0\n",
    "            \n",
    "            dataset_records.append(record)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(dataset_records)\n",
    "        \n",
    "        # Save as Parquet (Primary format - efficient for ML)\n",
    "        parquet_path = os.path.join(output_dir, 'breathing_dataset_features.parquet')\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "        print(f\"   ✅ Features saved as Parquet: {parquet_path}\")\n",
    "        \n",
    "        # Save as CSV (Human-readable format)\n",
    "        csv_path = os.path.join(output_dir, 'breathing_dataset_features.csv')\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"   ✅ Features saved as CSV: {csv_path}\")\n",
    "        \n",
    "        # Save raw time series data as Pickle (for full signal access)\n",
    "        pickle_path = os.path.join(output_dir, 'breathing_dataset_raw.pkl')\n",
    "        with open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(windows, f)\n",
    "        print(f\"   ✅ Raw time series saved as Pickle: {pickle_path}\")\n",
    "        \n",
    "        print(f\"📊 Dataset formats saved:\")\n",
    "        print(f\"   • Parquet: {os.path.getsize(parquet_path):,} bytes (recommended for ML)\")\n",
    "        print(f\"   • CSV: {os.path.getsize(csv_path):,} bytes (human-readable)\")\n",
    "        print(f\"   • Pickle: {os.path.getsize(pickle_path):,} bytes (full time series)\")\n",
    "\n",
    "# Add methods to the DatasetCreator class\n",
    "DatasetCreator.create_dataset = create_dataset\n",
    "DatasetCreator.save_dataset = save_dataset\n",
    "\n",
    "print(\"✅ Dataset creation and saving methods added!\")\n",
    "print(\"💾 Ready to create and save ML-ready datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ac2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for dataset creation - can be called from command line or notebook.\n",
    "    \"\"\"\n",
    "    # Default parameters\n",
    "    default_input_dir = \"../Data\"\n",
    "    default_output_dir = \"../Dataset\"\n",
    "    \n",
    "    # Check if running in notebook or command line\n",
    "    try:\n",
    "        # If running in Jupyter notebook\n",
    "        input_dir = default_input_dir\n",
    "        output_dir = default_output_dir\n",
    "        \n",
    "        print(\"🚀 BREATHING ANALYSIS DATASET CREATION\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"📥 Input directory: {input_dir}\")\n",
    "        print(f\"📤 Output directory: {output_dir}\")\n",
    "        print(f\"⏱️  Window size: 30 seconds with 50% overlap\")\n",
    "        print(f\"🏷️  Target labels: Hypopnea, Obstructive Apnea, Normal\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Create dataset creator\n",
    "        creator = DatasetCreator(window_duration=30, overlap_ratio=0.5, sampling_rate=32)\n",
    "        \n",
    "        # Create the dataset\n",
    "        stats = creator.create_dataset(input_dir, output_dir)\n",
    "        \n",
    "        if stats:\n",
    "            print(f\"\\n🎉 Dataset creation completed successfully!\")\n",
    "            print(f\"📊 Total windows: {stats.get('total_windows', 0):,}\")\n",
    "            print(f\"👥 Participants: {stats.get('participants', 0)}\")\n",
    "            print(f\"📁 Output saved to: {output_dir}/\")\n",
    "            \n",
    "            return stats\n",
    "        else:\n",
    "            print(\"❌ Dataset creation failed\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in dataset creation: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def create_dataset_cli():\n",
    "    \"\"\"\n",
    "    Command line interface for dataset creation.\n",
    "    Usage: python create_dataset.py -in_dir \"Data\" -out_dir \"Dataset\"\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Create ML dataset from sleep study recordings')\n",
    "    parser.add_argument('-in_dir', '--input_dir', default='../Data',\n",
    "                       help='Input directory containing participant folders')\n",
    "    parser.add_argument('-out_dir', '--output_dir', default='../Dataset',\n",
    "                       help='Output directory for saving the dataset')\n",
    "    parser.add_argument('--window_duration', type=int, default=30,\n",
    "                       help='Window duration in seconds (default: 30)')\n",
    "    parser.add_argument('--overlap_ratio', type=float, default=0.5,\n",
    "                       help='Overlap ratio between windows (default: 0.5)')\n",
    "    parser.add_argument('--sampling_rate', type=int, default=32,\n",
    "                       help='Sampling rate in Hz (default: 32)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(\"🚀 BREATHING ANALYSIS DATASET CREATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📥 Input directory: {args.input_dir}\")\n",
    "    print(f\"📤 Output directory: {args.output_dir}\")\n",
    "    print(f\"⏱️  Window size: {args.window_duration} seconds\")\n",
    "    print(f\"🔄 Overlap ratio: {args.overlap_ratio * 100}%\")\n",
    "    print(f\"📊 Sampling rate: {args.sampling_rate} Hz\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create dataset creator\n",
    "    creator = DatasetCreator(\n",
    "        window_duration=args.window_duration,\n",
    "        overlap_ratio=args.overlap_ratio,\n",
    "        sampling_rate=args.sampling_rate\n",
    "    )\n",
    "    \n",
    "    # Create the dataset\n",
    "    stats = creator.create_dataset(args.input_dir, args.output_dir)\n",
    "    \n",
    "    if stats:\n",
    "        print(f\"\\n🎉 Dataset creation completed successfully!\")\n",
    "        return 0\n",
    "    else:\n",
    "        print(\"❌ Dataset creation failed\")\n",
    "        return 1\n",
    "\n",
    "# Usage instructions\n",
    "print(\"🎯 USAGE INSTRUCTIONS:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"# From Jupyter notebook:\")\n",
    "print(\"# stats = main()\")\n",
    "print(\"# \")\n",
    "print(\"# From command line:\")\n",
    "print(\"# python create_dataset.py -in_dir '../Data' -out_dir '../Dataset'\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f7bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMONSTRATION: Create Dataset from Sleep Study Data\n",
    "print(\"🚀 DEMONSTRATION: Creating ML Dataset from Sleep Study Recordings\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Execute the main dataset creation function\n",
    "dataset_stats = main()\n",
    "\n",
    "if dataset_stats:\n",
    "    print(\"\\n📊 DATASET CREATION RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"✅ Success! Created {dataset_stats['total_windows']:,} labeled windows\")\n",
    "    print(f\"👥 Processed {dataset_stats['participants']} participants\")\n",
    "    print(f\"⏱️  Window specifications:\")\n",
    "    print(f\"   • Duration: {dataset_stats['window_duration_seconds']} seconds\")\n",
    "    print(f\"   • Overlap: {dataset_stats['overlap_ratio']*100}%\")\n",
    "    \n",
    "    print(f\"\\n🏷️  Label Distribution:\")\n",
    "    for label, count in dataset_stats['overall_distribution'].items():\n",
    "        percentage = (count / dataset_stats['total_windows']) * 100\n",
    "        print(f\"   • {label}: {count:,} windows ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n📁 Output Files Created:\")\n",
    "    print(f\"   • breathing_dataset_features.parquet (ML-ready features)\")\n",
    "    print(f\"   • breathing_dataset_features.csv (human-readable)\")\n",
    "    print(f\"   • breathing_dataset_raw.pkl (full time series)\")\n",
    "    print(f\"   • dataset_stats.json (creation statistics)\")\n",
    "    \n",
    "    print(f\"\\n💡 Format Choice Explanation:\")\n",
    "    print(f\"   🎯 Parquet: Primary format for ML training\")\n",
    "    print(f\"      • Fast loading and efficient storage\")\n",
    "    print(f\"      • Column-oriented, optimized for analytics\")\n",
    "    print(f\"      • Native support in pandas, scikit-learn\")\n",
    "    print(f\"   📄 CSV: Human-readable backup format\")\n",
    "    print(f\"      • Easy inspection and sharing\")\n",
    "    print(f\"      • Compatible with any tool\")\n",
    "    print(f\"   🗂️  Pickle: Full time series preservation\")\n",
    "    print(f\"      • Complete signal data for advanced analysis\")\n",
    "    print(f\"      • Python-native complex object storage\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✅ DATASET CREATION COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"🚀 Ready for machine learning model training!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Dataset creation failed - check error messages above\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
