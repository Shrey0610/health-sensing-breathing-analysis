{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03021ca2",
   "metadata": {},
   "source": [
    "# Dataset Creation for Health Sensing Breathing Analysis 📊\n",
    "\n",
    "## Machine Learning Dataset Generation\n",
    "\n",
    "### Task Overview\n",
    "Create a labeled dataset from 8-hour sleep study recordings by splitting continuous signals into 30-second windows with 50% overlap. Each window will be labeled based on breathing events (Hypopnea, Obstructive Apnea, or Normal).\n",
    "\n",
    "### Key Requirements ✅\n",
    "- **Window Size**: 30-second segments with 50% overlap (15-second step)\n",
    "- **Label Assignment**: Based on >50% overlap with breathing events\n",
    "- **Target Labels**: Hypopnea, Obstructive Apnea, Normal\n",
    "- **Input Signals**: Nasal Airflow, Thoracic Movement, SpO₂\n",
    "- **Output Format**: Efficient storage for ML training\n",
    "\n",
    "### Technical Approach\n",
    "- **Sliding Window**: Extract overlapping time segments from continuous signals\n",
    "- **Event Mapping**: Assign labels based on temporal overlap with annotations\n",
    "- **Data Format**: Parquet format for efficient storage and fast loading\n",
    "- **Feature Engineering**: Time-series windows ready for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44fadbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n",
      "📊 Ready for dataset creation from sleep study data\n",
      "🎯 Target: 30-second windows with event-based labeling\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For data storage\n",
    "import pickle\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(\"📊 Ready for dataset creation from sleep study data\")\n",
    "print(\"🎯 Target: 30-second windows with event-based labeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85fb9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCreator:\n",
    "    \"\"\"\n",
    "    Creates labeled dataset from continuous sleep study recordings.\n",
    "    Splits 8-hour signals into 30-second windows with 50% overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_duration=30, overlap_ratio=0.5, sampling_rate=32):\n",
    "        \"\"\"\n",
    "        Initialize the dataset creator.\n",
    "        \n",
    "        Args:\n",
    "            window_duration (int): Window size in seconds (default: 30)\n",
    "            overlap_ratio (float): Overlap ratio between windows (default: 0.5 for 50%)\n",
    "            sampling_rate (int): Sampling rate in Hz (default: 32)\n",
    "        \"\"\"\n",
    "        self.window_duration = window_duration\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.sampling_rate = sampling_rate\n",
    "        \n",
    "        # Calculate window parameters\n",
    "        self.window_samples = window_duration * sampling_rate  # 30 * 32 = 960 samples\n",
    "        self.step_samples = int(self.window_samples * (1 - overlap_ratio))  # 480 samples (15 seconds)\n",
    "        \n",
    "        # Target labels for the dataset\n",
    "        self.target_labels = ['Hypopnea', 'Obstructive Apnea', 'Normal']\n",
    "        \n",
    "        print(f\"✅ DatasetCreator initialized\")\n",
    "        print(f\"   ⏱️  Window duration: {window_duration} seconds ({self.window_samples} samples)\")\n",
    "        print(f\"   🔄 Overlap: {overlap_ratio*100}% ({window_duration * overlap_ratio} seconds)\")\n",
    "        print(f\"   👣 Step size: {self.step_samples} samples ({self.step_samples/sampling_rate} seconds)\")\n",
    "        print(f\"   🏷️  Target labels: {self.target_labels}\")\n",
    "    \n",
    "    def parse_datetime(self, date_str):\n",
    "        \"\"\"Parse datetime strings from various formats found in the data files.\"\"\"\n",
    "        formats = [\n",
    "            \"%d.%m.%Y %H:%M:%S,%f\",  # 30.05.2024 20:59:00,000\n",
    "            \"%d.%m.%Y %H:%M:%S\",     # 30.05.2024 20:59:00\n",
    "            \"%d-%m-%Y %H:%M:%S,%f\",  # 30-05-2024 21:22:45,000\n",
    "            \"%d-%m-%Y %H:%M:%S\",     # 30-05-2024 21:22:45\n",
    "            \"%d_%m_%Y %H:%M:%S,%f\",  # Alternative format\n",
    "            \"%m/%d/%Y %I:%M:%S %p\",  # 5/30/2024 8:59:00 PM\n",
    "        ]\n",
    "        \n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                return datetime.strptime(date_str.strip(), fmt)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        # Manual parsing for edge cases\n",
    "        try:\n",
    "            clean_str = date_str.replace(';', '').strip()\n",
    "            if ',' in clean_str:\n",
    "                dt_part, ms_part = clean_str.rsplit(',', 1)\n",
    "                for base_fmt in [\"%d.%m.%Y %H:%M:%S\", \"%d-%m-%Y %H:%M:%S\"]:\n",
    "                    try:\n",
    "                        dt = datetime.strptime(dt_part, base_fmt)\n",
    "                        ms = int(ms_part)\n",
    "                        return dt + timedelta(milliseconds=ms)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            else:\n",
    "                for base_fmt in [\"%d.%m.%Y %H:%M:%S\", \"%d-%m-%Y %H:%M:%S\"]:\n",
    "                    try:\n",
    "                        return datetime.strptime(clean_str, base_fmt)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def find_file_by_pattern(self, folder, patterns):\n",
    "        \"\"\"Find files matching any of the given patterns.\"\"\"\n",
    "        for pattern in patterns:\n",
    "            files = glob.glob(os.path.join(folder, pattern))\n",
    "            if files:\n",
    "                return files[0]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd6cc332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Signal loading methods added!\n",
      "📥 Ready to load signals and events from participant folders\n"
     ]
    }
   ],
   "source": [
    "    def load_signal_data(self, file_path, signal_type):\n",
    "        \"\"\"Load signal data with timestamps from a file.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"⚠️  Warning: Signal file not found: {os.path.basename(file_path)}\")\n",
    "            return None\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Extract start time from header\n",
    "        start_time_line = None\n",
    "        for line in lines[:4]:\n",
    "            if 'Start Time:' in line:\n",
    "                start_time_line = line\n",
    "                break\n",
    "        \n",
    "        if not start_time_line:\n",
    "            print(f\"⚠️  Warning: No start time found in {os.path.basename(file_path)}\")\n",
    "            return None\n",
    "        \n",
    "        start_time_str = start_time_line.split('Start Time:')[1].strip()\n",
    "        start_time = self.parse_datetime(start_time_str)\n",
    "        \n",
    "        if not start_time:\n",
    "            print(f\"⚠️  Warning: Could not parse start time: {start_time_str}\")\n",
    "            return None\n",
    "        \n",
    "        # Parse data lines\n",
    "        values = []\n",
    "        timestamps = []\n",
    "        \n",
    "        for line in lines[4:]:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                if ';' in line:\n",
    "                    time_str, value_str = line.split(';', 1)\n",
    "                else:\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 2:\n",
    "                        time_str, value_str = parts[0], parts[1]\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                timestamp = self.parse_datetime(time_str.strip())\n",
    "                if timestamp:\n",
    "                    timestamps.append(timestamp)\n",
    "                    values.append(float(value_str.strip()))\n",
    "                    \n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "        \n",
    "        if not timestamps:\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'timestamp': timestamps,\n",
    "            'value': values,\n",
    "            'signal_type': signal_type\n",
    "        })\n",
    "        \n",
    "        df = df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')\n",
    "        df = df.set_index('timestamp')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def load_events_data(self, file_path):\n",
    "        \"\"\"Load breathing events data.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            return None\n",
    "            \n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        events = []\n",
    "        data_lines = [line.strip() for line in lines[4:] if line.strip()]\n",
    "        \n",
    "        for line in data_lines:\n",
    "            try:\n",
    "                main_parts = line.split(';')\n",
    "                if len(main_parts) < 3:\n",
    "                    continue\n",
    "                    \n",
    "                time_range = main_parts[0].strip()\n",
    "                duration = float(main_parts[1].strip())\n",
    "                event_type = main_parts[2].strip()\n",
    "                sleep_stage = main_parts[3].strip() if len(main_parts) > 3 else \"\"\n",
    "                \n",
    "                # Parse time range\n",
    "                dash_pos = time_range.rfind('-')\n",
    "                if dash_pos < 0:\n",
    "                    continue\n",
    "                    \n",
    "                start_str = time_range[:dash_pos].strip()\n",
    "                end_time_part = time_range[dash_pos+1:].strip()\n",
    "                \n",
    "                date_part = start_str.split(' ')[0]\n",
    "                end_str = f\"{date_part} {end_time_part}\"\n",
    "                \n",
    "                start_time = self.parse_datetime(start_str)\n",
    "                end_time = self.parse_datetime(end_str)\n",
    "                \n",
    "                if start_time and end_time:\n",
    "                    events.append({\n",
    "                        'start': start_time,\n",
    "                        'end': end_time,\n",
    "                        'duration': duration,\n",
    "                        'event_type': event_type,\n",
    "                        'sleep_stage': sleep_stage\n",
    "                    })\n",
    "            except Exception:\n",
    "                continue\n",
    "                \n",
    "        if events:\n",
    "            return pd.DataFrame(events)\n",
    "        return None\n",
    "\n",
    "# Add these methods to the DatasetCreator class\n",
    "DatasetCreator.load_signal_data = load_signal_data\n",
    "DatasetCreator.load_events_data = load_events_data\n",
    "\n",
    "print(\"✅ Signal loading methods added!\")\n",
    "print(\"📥 Ready to load signals and events from participant folders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61e84cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Window extraction and labeling methods added!\n",
      "🪟 Ready to create 30-second windows with event-based labels\n"
     ]
    }
   ],
   "source": [
    "    def extract_windows(self, signal_data, signal_name):\n",
    "        \"\"\"\n",
    "        Extract overlapping 30-second windows from signal data.\n",
    "        \n",
    "        Args:\n",
    "            signal_data (DataFrame): Signal data with timestamp index\n",
    "            signal_name (str): Name of the signal\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples: (window_start_time, window_end_time, window_values)\n",
    "        \"\"\"\n",
    "        if signal_data is None or len(signal_data) < self.window_samples:\n",
    "            return []\n",
    "        \n",
    "        windows = []\n",
    "        values = signal_data['value'].values\n",
    "        timestamps = signal_data.index\n",
    "        \n",
    "        # Extract overlapping windows\n",
    "        for start_idx in range(0, len(values) - self.window_samples + 1, self.step_samples):\n",
    "            end_idx = start_idx + self.window_samples\n",
    "            \n",
    "            window_values = values[start_idx:end_idx]\n",
    "            window_start_time = timestamps[start_idx]\n",
    "            window_end_time = timestamps[end_idx - 1]\n",
    "            \n",
    "            windows.append((window_start_time, window_end_time, window_values))\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def assign_label(self, window_start, window_end, events_data):\n",
    "        \"\"\"\n",
    "        Assign label to a window based on overlap with breathing events.\n",
    "        \n",
    "        Args:\n",
    "            window_start (datetime): Window start time\n",
    "            window_end (datetime): Window end time\n",
    "            events_data (DataFrame): Events data\n",
    "            \n",
    "        Returns:\n",
    "            str: Label for the window ('Hypopnea', 'Obstructive Apnea', or 'Normal')\n",
    "        \"\"\"\n",
    "        if events_data is None or len(events_data) == 0:\n",
    "            return 'Normal'\n",
    "        \n",
    "        window_duration = (window_end - window_start).total_seconds()\n",
    "        \n",
    "        # Check overlap with each event\n",
    "        for _, event in events_data.iterrows():\n",
    "            event_type = event['event_type']\n",
    "            \n",
    "            # Only consider target labels\n",
    "            if event_type not in ['Hypopnea', 'Obstructive Apnea']:\n",
    "                continue\n",
    "            \n",
    "            event_start = event['start']\n",
    "            event_end = event['end']\n",
    "            \n",
    "            # Calculate overlap\n",
    "            overlap_start = max(window_start, event_start)\n",
    "            overlap_end = min(window_end, event_end)\n",
    "            \n",
    "            if overlap_start < overlap_end:\n",
    "                overlap_duration = (overlap_end - overlap_start).total_seconds()\n",
    "                overlap_ratio = overlap_duration / window_duration\n",
    "                \n",
    "                # If more than 50% overlap, assign the event label\n",
    "                if overlap_ratio > 0.5:\n",
    "                    return event_type\n",
    "        \n",
    "        return 'Normal'\n",
    "    \n",
    "    def process_participant(self, participant_folder):\n",
    "        \"\"\"\n",
    "        Process a single participant to extract labeled windows.\n",
    "        \n",
    "        Args:\n",
    "            participant_folder (str): Path to participant folder\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries: Windows with features and labels\n",
    "        \"\"\"\n",
    "        participant_id = os.path.basename(participant_folder)\n",
    "        print(f\"🔄 Processing {participant_id}...\")\n",
    "        \n",
    "        # Define file patterns for different naming conventions\n",
    "        flow_patterns = [\n",
    "            \"Flow - *.txt\", \"Flow  - *.txt\", \"Flow Signal - *.txt\", \"Flow Nasal - *.txt\"\n",
    "        ]\n",
    "        thorac_patterns = [\n",
    "            \"Thorac - *.txt\", \"Thorac  - *.txt\", \"Thorac Signal - *.txt\", \"Thorac Movement - *.txt\"\n",
    "        ]\n",
    "        spo2_patterns = [\n",
    "            \"SPO2 - *.txt\", \"SPO2  - *.txt\", \"SPO2 Signal - *.txt\"\n",
    "        ]\n",
    "        events_patterns = [\n",
    "            \"Flow Events - *.txt\", \"Flow Events  - *.txt\"\n",
    "        ]\n",
    "        \n",
    "        # Load signals\n",
    "        signals = {}\n",
    "        \n",
    "        flow_file = self.find_file_by_pattern(participant_folder, flow_patterns)\n",
    "        if flow_file:\n",
    "            signals['nasal_airflow'] = self.load_signal_data(flow_file, 'Nasal Airflow')\n",
    "        \n",
    "        thorac_file = self.find_file_by_pattern(participant_folder, thorac_patterns)\n",
    "        if thorac_file:\n",
    "            signals['thoracic_movement'] = self.load_signal_data(thorac_file, 'Thoracic Movement')\n",
    "        \n",
    "        spo2_file = self.find_file_by_pattern(participant_folder, spo2_patterns)\n",
    "        if spo2_file:\n",
    "            signals['spo2'] = self.load_signal_data(spo2_file, 'SpO₂')\n",
    "        \n",
    "        # Load events\n",
    "        events_file = self.find_file_by_pattern(participant_folder, events_patterns)\n",
    "        events_data = self.load_events_data(events_file) if events_file else None\n",
    "        \n",
    "        # Filter events to only target labels\n",
    "        if events_data is not None:\n",
    "            target_events = events_data[events_data['event_type'].isin(['Hypopnea', 'Obstructive Apnea'])]\n",
    "        else:\n",
    "            target_events = None\n",
    "        \n",
    "        print(f\"   📊 Loaded signals: {list(signals.keys())}\")\n",
    "        if target_events is not None:\n",
    "            event_counts = target_events['event_type'].value_counts()\n",
    "            print(f\"   🚨 Events: {dict(event_counts)}\")\n",
    "        else:\n",
    "            print(f\"   🚨 Events: None found\")\n",
    "        \n",
    "        # Extract windows from the primary signal (nasal airflow)\n",
    "        if 'nasal_airflow' not in signals or signals['nasal_airflow'] is None:\n",
    "            print(f\"   ❌ No nasal airflow data found for {participant_id}\")\n",
    "            return []\n",
    "        \n",
    "        primary_signal = signals['nasal_airflow']\n",
    "        windows = self.extract_windows(primary_signal, 'nasal_airflow')\n",
    "        \n",
    "        print(f\"   🪟 Extracted {len(windows)} windows\")\n",
    "        \n",
    "        # Process each window\n",
    "        dataset_windows = []\n",
    "        \n",
    "        for i, (window_start, window_end, nasal_values) in enumerate(windows):\n",
    "            # Get corresponding values from other signals\n",
    "            thorac_values = self._get_window_values(signals.get('thoracic_movement'), \n",
    "                                                   window_start, window_end)\n",
    "            spo2_values = self._get_window_values(signals.get('spo2'), \n",
    "                                                 window_start, window_end)\n",
    "            \n",
    "            # Assign label\n",
    "            label = self.assign_label(window_start, window_end, target_events)\n",
    "            \n",
    "            # Create window data\n",
    "            window_data = {\n",
    "                'participant_id': participant_id,\n",
    "                'window_id': f\"{participant_id}_W{i:04d}\",\n",
    "                'start_time': window_start,\n",
    "                'end_time': window_end,\n",
    "                'duration': self.window_duration,\n",
    "                'label': label,\n",
    "                'nasal_airflow': nasal_values,\n",
    "                'thoracic_movement': thorac_values,\n",
    "                'spo2': spo2_values\n",
    "            }\n",
    "            \n",
    "            dataset_windows.append(window_data)\n",
    "        \n",
    "        # Label distribution\n",
    "        labels = [w['label'] for w in dataset_windows]\n",
    "        label_counts = pd.Series(labels).value_counts()\n",
    "        print(f\"   🏷️  Label distribution: {dict(label_counts)}\")\n",
    "        \n",
    "        return dataset_windows\n",
    "    \n",
    "    def _get_window_values(self, signal_data, window_start, window_end):\n",
    "        \"\"\"Get signal values for a specific time window.\"\"\"\n",
    "        if signal_data is None:\n",
    "            return None\n",
    "        \n",
    "        # Filter signal data for the window time range\n",
    "        window_data = signal_data[(signal_data.index >= window_start) & \n",
    "                                 (signal_data.index <= window_end)]\n",
    "        \n",
    "        if len(window_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        return window_data['value'].values\n",
    "\n",
    "# Add methods to the DatasetCreator class\n",
    "DatasetCreator.extract_windows = extract_windows\n",
    "DatasetCreator.assign_label = assign_label\n",
    "DatasetCreator.process_participant = process_participant\n",
    "DatasetCreator._get_window_values = _get_window_values\n",
    "\n",
    "print(\"✅ Window extraction and labeling methods added!\")\n",
    "print(\"🪟 Ready to create 30-second windows with event-based labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a62692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset creation and saving methods added!\n",
      "💾 Ready to create and save ML-ready datasets\n"
     ]
    }
   ],
   "source": [
    "    def create_dataset(self, input_dir, output_dir):\n",
    "        \"\"\"\n",
    "        Create the complete dataset from all participants.\n",
    "        \n",
    "        Args:\n",
    "            input_dir (str): Input directory containing participant folders\n",
    "            output_dir (str): Output directory for saving the dataset\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dataset creation summary\n",
    "        \"\"\"\n",
    "        print(\"🚀 DATASET CREATION: Processing all participants\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Find all participant folders\n",
    "        participants = [f for f in os.listdir(input_dir) \n",
    "                       if os.path.isdir(os.path.join(input_dir, f)) and f.startswith('AP')]\n",
    "        participants.sort()\n",
    "        \n",
    "        print(f\"📁 Found {len(participants)} participants: {participants}\")\n",
    "        \n",
    "        all_windows = []\n",
    "        participant_stats = {}\n",
    "        \n",
    "        # Process each participant\n",
    "        for i, participant in enumerate(participants, 1):\n",
    "            print(f\"\\n[{i}/{len(participants)}] Processing {participant}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            participant_path = os.path.join(input_dir, participant)\n",
    "            \n",
    "            try:\n",
    "                windows = self.process_participant(participant_path)\n",
    "                \n",
    "                if windows:\n",
    "                    all_windows.extend(windows)\n",
    "                    \n",
    "                    # Calculate statistics\n",
    "                    labels = [w['label'] for w in windows]\n",
    "                    participant_stats[participant] = {\n",
    "                        'total_windows': len(windows),\n",
    "                        'label_distribution': pd.Series(labels).value_counts().to_dict(),\n",
    "                        'duration_hours': len(windows) * self.window_duration / 3600,\n",
    "                        'status': 'success'\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"   ✅ {participant}: {len(windows)} windows created\")\n",
    "                else:\n",
    "                    participant_stats[participant] = {\n",
    "                        'total_windows': 0,\n",
    "                        'label_distribution': {},\n",
    "                        'duration_hours': 0,\n",
    "                        'status': 'failed'\n",
    "                    }\n",
    "                    print(f\"   ❌ {participant}: No windows created\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                participant_stats[participant] = {\n",
    "                    'total_windows': 0,\n",
    "                    'label_distribution': {},\n",
    "                    'duration_hours': 0,\n",
    "                    'status': 'error',\n",
    "                    'error': str(e)\n",
    "                }\n",
    "                print(f\"   ❌ {participant}: Error - {str(e)}\")\n",
    "        \n",
    "        # Dataset summary\n",
    "        print(f\"\\n\" + \"=\" * 70)\n",
    "        print(\"📋 DATASET CREATION SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        total_windows = len(all_windows)\n",
    "        if total_windows > 0:\n",
    "            # Overall label distribution\n",
    "            all_labels = [w['label'] for w in all_windows]\n",
    "            overall_distribution = pd.Series(all_labels).value_counts()\n",
    "            \n",
    "            print(f\"✅ Total windows created: {total_windows:,}\")\n",
    "            print(f\"📊 Overall label distribution:\")\n",
    "            for label, count in overall_distribution.items():\n",
    "                percentage = (count / total_windows) * 100\n",
    "                print(f\"   • {label}: {count:,} ({percentage:.1f}%)\")\n",
    "            \n",
    "            # Save dataset\n",
    "            print(f\"\\n💾 Saving dataset to: {output_dir}/\")\n",
    "            self.save_dataset(all_windows, output_dir)\n",
    "            \n",
    "            # Save statistics\n",
    "            stats_summary = {\n",
    "                'creation_date': datetime.now().isoformat(),\n",
    "                'total_windows': total_windows,\n",
    "                'participants': len(participants),\n",
    "                'window_duration_seconds': self.window_duration,\n",
    "                'overlap_ratio': self.overlap_ratio,\n",
    "                'overall_distribution': overall_distribution.to_dict(),\n",
    "                'participant_stats': participant_stats\n",
    "            }\n",
    "            \n",
    "            stats_path = os.path.join(output_dir, 'dataset_stats.json')\n",
    "            import json\n",
    "            with open(stats_path, 'w') as f:\n",
    "                # Convert datetime objects to strings for JSON serialization\n",
    "                def json_serializer(obj):\n",
    "                    if isinstance(obj, datetime):\n",
    "                        return obj.isoformat()\n",
    "                    return obj\n",
    "                \n",
    "                json.dump(stats_summary, f, indent=2, default=json_serializer)\n",
    "            \n",
    "            print(f\"📈 Statistics saved to: {stats_path}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ No windows were created from any participant\")\n",
    "            stats_summary = {}\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        return stats_summary\n",
    "    \n",
    "    def save_dataset(self, windows, output_dir):\n",
    "        \"\"\"\n",
    "        Save the dataset in multiple formats for different use cases.\n",
    "        \n",
    "        Args:\n",
    "            windows (list): List of window dictionaries\n",
    "            output_dir (str): Output directory\n",
    "        \"\"\"\n",
    "        print(\"💾 Saving dataset in multiple formats...\")\n",
    "        \n",
    "        # Prepare data for saving\n",
    "        dataset_records = []\n",
    "        \n",
    "        for window in windows:\n",
    "            # Create a flattened record for tabular formats\n",
    "            record = {\n",
    "                'participant_id': window['participant_id'],\n",
    "                'window_id': window['window_id'],\n",
    "                'start_time': window['start_time'],\n",
    "                'end_time': window['end_time'],\n",
    "                'duration': window['duration'],\n",
    "                'label': window['label']\n",
    "            }\n",
    "            \n",
    "            # Add signal features (basic statistics for now)\n",
    "            for signal_name in ['nasal_airflow', 'thoracic_movement', 'spo2']:\n",
    "                values = window[signal_name]\n",
    "                if values is not None and len(values) > 0:\n",
    "                    record[f'{signal_name}_mean'] = np.mean(values)\n",
    "                    record[f'{signal_name}_std'] = np.std(values)\n",
    "                    record[f'{signal_name}_min'] = np.min(values)\n",
    "                    record[f'{signal_name}_max'] = np.max(values)\n",
    "                    record[f'{signal_name}_samples'] = len(values)\n",
    "                else:\n",
    "                    record[f'{signal_name}_mean'] = None\n",
    "                    record[f'{signal_name}_std'] = None\n",
    "                    record[f'{signal_name}_min'] = None\n",
    "                    record[f'{signal_name}_max'] = None\n",
    "                    record[f'{signal_name}_samples'] = 0\n",
    "            \n",
    "            dataset_records.append(record)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(dataset_records)\n",
    "        \n",
    "        # Save as Parquet (Primary format - efficient for ML)\n",
    "        parquet_path = os.path.join(output_dir, 'breathing_dataset_features.parquet')\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "        print(f\"   ✅ Features saved as Parquet: {parquet_path}\")\n",
    "        \n",
    "        # Save as CSV (Human-readable format)\n",
    "        csv_path = os.path.join(output_dir, 'breathing_dataset_features.csv')\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"   ✅ Features saved as CSV: {csv_path}\")\n",
    "        \n",
    "        # Save raw time series data as Pickle (for full signal access)\n",
    "        pickle_path = os.path.join(output_dir, 'breathing_dataset_raw.pkl')\n",
    "        with open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(windows, f)\n",
    "        print(f\"   ✅ Raw time series saved as Pickle: {pickle_path}\")\n",
    "        \n",
    "        print(f\"📊 Dataset formats saved:\")\n",
    "        print(f\"   • Parquet: {os.path.getsize(parquet_path):,} bytes (recommended for ML)\")\n",
    "        print(f\"   • CSV: {os.path.getsize(csv_path):,} bytes (human-readable)\")\n",
    "        print(f\"   • Pickle: {os.path.getsize(pickle_path):,} bytes (full time series)\")\n",
    "\n",
    "# Add methods to the DatasetCreator class\n",
    "DatasetCreator.create_dataset = create_dataset\n",
    "DatasetCreator.save_dataset = save_dataset\n",
    "\n",
    "print(\"✅ Dataset creation and saving methods added!\")\n",
    "print(\"💾 Ready to create and save ML-ready datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a2ac2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 USAGE INSTRUCTIONS:\n",
      "==================================================\n",
      "# From Jupyter notebook:\n",
      "# stats = main()\n",
      "# \n",
      "# From command line:\n",
      "# python create_dataset.py -in_dir '../Data' -out_dir '../Dataset'\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for dataset creation - can be called from command line or notebook.\n",
    "    \"\"\"\n",
    "    # Default parameters\n",
    "    default_input_dir = \"../Data\"\n",
    "    default_output_dir = \"../Dataset\"\n",
    "    \n",
    "    # Check if running in notebook or command line\n",
    "    try:\n",
    "        # If running in Jupyter notebook\n",
    "        input_dir = default_input_dir\n",
    "        output_dir = default_output_dir\n",
    "        \n",
    "        print(\"🚀 BREATHING ANALYSIS DATASET CREATION\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"📥 Input directory: {input_dir}\")\n",
    "        print(f\"📤 Output directory: {output_dir}\")\n",
    "        print(f\"⏱️  Window size: 30 seconds with 50% overlap\")\n",
    "        print(f\"🏷️  Target labels: Hypopnea, Obstructive Apnea, Normal\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Create dataset creator\n",
    "        creator = DatasetCreator(window_duration=30, overlap_ratio=0.5, sampling_rate=32)\n",
    "        \n",
    "        # Create the dataset\n",
    "        stats = creator.create_dataset(input_dir, output_dir)\n",
    "        \n",
    "        if stats:\n",
    "            print(f\"\\n🎉 Dataset creation completed successfully!\")\n",
    "            print(f\"📊 Total windows: {stats.get('total_windows', 0):,}\")\n",
    "            print(f\"👥 Participants: {stats.get('participants', 0)}\")\n",
    "            print(f\"📁 Output saved to: {output_dir}/\")\n",
    "            \n",
    "            return stats\n",
    "        else:\n",
    "            print(\"❌ Dataset creation failed\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in dataset creation: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def create_dataset_cli():\n",
    "    \"\"\"\n",
    "    Command line interface for dataset creation.\n",
    "    Usage: python create_dataset.py -in_dir \"Data\" -out_dir \"Dataset\"\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Create ML dataset from sleep study recordings')\n",
    "    parser.add_argument('-in_dir', '--input_dir', default='../Data',\n",
    "                       help='Input directory containing participant folders')\n",
    "    parser.add_argument('-out_dir', '--output_dir', default='../Dataset',\n",
    "                       help='Output directory for saving the dataset')\n",
    "    parser.add_argument('--window_duration', type=int, default=30,\n",
    "                       help='Window duration in seconds (default: 30)')\n",
    "    parser.add_argument('--overlap_ratio', type=float, default=0.5,\n",
    "                       help='Overlap ratio between windows (default: 0.5)')\n",
    "    parser.add_argument('--sampling_rate', type=int, default=32,\n",
    "                       help='Sampling rate in Hz (default: 32)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(\"🚀 BREATHING ANALYSIS DATASET CREATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📥 Input directory: {args.input_dir}\")\n",
    "    print(f\"📤 Output directory: {args.output_dir}\")\n",
    "    print(f\"⏱️  Window size: {args.window_duration} seconds\")\n",
    "    print(f\"🔄 Overlap ratio: {args.overlap_ratio * 100}%\")\n",
    "    print(f\"📊 Sampling rate: {args.sampling_rate} Hz\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create dataset creator\n",
    "    creator = DatasetCreator(\n",
    "        window_duration=args.window_duration,\n",
    "        overlap_ratio=args.overlap_ratio,\n",
    "        sampling_rate=args.sampling_rate\n",
    "    )\n",
    "    \n",
    "    # Create the dataset\n",
    "    stats = creator.create_dataset(args.input_dir, args.output_dir)\n",
    "    \n",
    "    if stats:\n",
    "        print(f\"\\n🎉 Dataset creation completed successfully!\")\n",
    "        return 0\n",
    "    else:\n",
    "        print(\"❌ Dataset creation failed\")\n",
    "        return 1\n",
    "\n",
    "# Usage instructions\n",
    "print(\"🎯 USAGE INSTRUCTIONS:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"# From Jupyter notebook:\")\n",
    "print(\"# stats = main()\")\n",
    "print(\"# \")\n",
    "print(\"# From command line:\")\n",
    "print(\"# python create_dataset.py -in_dir '../Data' -out_dir '../Dataset'\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f7bb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 DEMONSTRATION: Creating ML Dataset from Sleep Study Recordings\n",
      "================================================================================\n",
      "🚀 BREATHING ANALYSIS DATASET CREATION\n",
      "============================================================\n",
      "📥 Input directory: ../Data\n",
      "📤 Output directory: ../Dataset\n",
      "⏱️  Window size: 30 seconds with 50% overlap\n",
      "🏷️  Target labels: Hypopnea, Obstructive Apnea, Normal\n",
      "============================================================\n",
      "❌ Error in dataset creation: name 'window_samples' is not defined\n",
      "❌ Dataset creation failed - check error messages above\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 DEMONSTRATION: Creating ML Dataset from Sleep Study Recordings\n",
      "================================================================================\n",
      "🚀 BREATHING ANALYSIS DATASET CREATION\n",
      "============================================================\n",
      "📥 Input directory: ../Data\n",
      "📤 Output directory: ../Dataset\n",
      "⏱️  Window size: 30 seconds with 50% overlap\n",
      "🏷️  Target labels: Hypopnea, Obstructive Apnea, Normal\n",
      "============================================================\n",
      "❌ Error in dataset creation: name 'window_samples' is not defined\n",
      "❌ Dataset creation failed - check error messages above\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cb/19hy99v14gxg9n0mbtk79djc0000gn/T/ipykernel_60001/1355396864.py\", line 24, in main\n",
      "    creator = DatasetCreator(window_duration=30, overlap_ratio=0.5, sampling_rate=32)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/cb/19hy99v14gxg9n0mbtk79djc0000gn/T/ipykernel_60001/4230111827.py\", line 22, in __init__\n",
      "    self.step_samples = int(window_samples * (1 - overlap_ratio))  # 480 samples (15 seconds)\n",
      "                            ^^^^^^^^^^^^^^\n",
      "NameError: name 'window_samples' is not defined\n"
     ]
    }
   ],
   "source": [
    "# DEMONSTRATION: Create Dataset from Sleep Study Data\n",
    "print(\"🚀 DEMONSTRATION: Creating ML Dataset from Sleep Study Recordings\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Execute the main dataset creation function\n",
    "dataset_stats = main()\n",
    "\n",
    "if dataset_stats:\n",
    "    print(\"\\n📊 DATASET CREATION RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"✅ Success! Created {dataset_stats['total_windows']:,} labeled windows\")\n",
    "    print(f\"👥 Processed {dataset_stats['participants']} participants\")\n",
    "    print(f\"⏱️  Window specifications:\")\n",
    "    print(f\"   • Duration: {dataset_stats['window_duration_seconds']} seconds\")\n",
    "    print(f\"   • Overlap: {dataset_stats['overlap_ratio']*100}%\")\n",
    "    \n",
    "    print(f\"\\n🏷️  Label Distribution:\")\n",
    "    for label, count in dataset_stats['overall_distribution'].items():\n",
    "        percentage = (count / dataset_stats['total_windows']) * 100\n",
    "        print(f\"   • {label}: {count:,} windows ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n📁 Output Files Created:\")\n",
    "    print(f\"   • breathing_dataset_features.parquet (ML-ready features)\")\n",
    "    print(f\"   • breathing_dataset_features.csv (human-readable)\")\n",
    "    print(f\"   • breathing_dataset_raw.pkl (full time series)\")\n",
    "    print(f\"   • dataset_stats.json (creation statistics)\")\n",
    "    \n",
    "    print(f\"\\n💡 Format Choice Explanation:\")\n",
    "    print(f\"   🎯 Parquet: Primary format for ML training\")\n",
    "    print(f\"      • Fast loading and efficient storage\")\n",
    "    print(f\"      • Column-oriented, optimized for analytics\")\n",
    "    print(f\"      • Native support in pandas, scikit-learn\")\n",
    "    print(f\"   📄 CSV: Human-readable backup format\")\n",
    "    print(f\"      • Easy inspection and sharing\")\n",
    "    print(f\"      • Compatible with any tool\")\n",
    "    print(f\"   🗂️  Pickle: Full time series preservation\")\n",
    "    print(f\"      • Complete signal data for advanced analysis\")\n",
    "    print(f\"      • Python-native complex object storage\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✅ DATASET CREATION COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"🚀 Ready for machine learning model training!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Dataset creation failed - check error messages above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d4eacf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 CREATING DATASET FROM SLEEP STUDY DATA\n",
      "============================================================\n",
      "✅ DatasetCreator initialized\n",
      "   ⏱️  Window duration: 30 seconds (960 samples)\n",
      "   🔄 Overlap: 50.0% (15.0 seconds)\n",
      "   👣 Step size: 480 samples (15.0 seconds)\n",
      "   🏷️  Target labels: ['Hypopnea', 'Obstructive Apnea', 'Normal']\n",
      "📥 Input directory: ../Data\n",
      "📤 Output directory: ../Dataset\n",
      "\n",
      "🔄 Starting dataset creation...\n",
      "🚀 DATASET CREATION: Processing all participants\n",
      "======================================================================\n",
      "📁 Found 5 participants: ['AP01', 'AP02', 'AP03', 'AP04', 'AP05']\n",
      "\n",
      "[1/5] Processing AP01\n",
      "--------------------------------------------------\n",
      "🔄 Processing AP01...\n",
      "   📊 Loaded signals: ['nasal_airflow', 'thoracic_movement', 'spo2']\n",
      "   🚨 Events: {'Hypopnea': 125, 'Obstructive Apnea': 36}\n",
      "   🪟 Extracted 1822 windows\n",
      "   📊 Loaded signals: ['nasal_airflow', 'thoracic_movement', 'spo2']\n",
      "   🚨 Events: {'Hypopnea': 125, 'Obstructive Apnea': 36}\n",
      "   🪟 Extracted 1822 windows\n",
      "   🏷️  Label distribution: {'Normal': 1727, 'Hypopnea': 79, 'Obstructive Apnea': 16}\n",
      "   ✅ AP01: 1822 windows created\n",
      "\n",
      "[2/5] Processing AP02\n",
      "--------------------------------------------------\n",
      "🔄 Processing AP02...\n",
      "   🏷️  Label distribution: {'Normal': 1727, 'Hypopnea': 79, 'Obstructive Apnea': 16}\n",
      "   ✅ AP01: 1822 windows created\n",
      "\n",
      "[2/5] Processing AP02\n",
      "--------------------------------------------------\n",
      "🔄 Processing AP02...\n",
      "   📊 Loaded signals: ['nasal_airflow', 'thoracic_movement', 'spo2']\n",
      "   🚨 Events: {'Hypopnea': 181, 'Obstructive Apnea': 5}\n",
      "   🪟 Extracted 1769 windows\n",
      "   📊 Loaded signals: ['nasal_airflow', 'thoracic_movement', 'spo2']\n",
      "   🚨 Events: {'Hypopnea': 181, 'Obstructive Apnea': 5}\n",
      "   🪟 Extracted 1769 windows\n",
      "   🏷️  Label distribution: {'Normal': 1616, 'Hypopnea': 150, 'Obstructive Apnea': 3}\n",
      "   ✅ AP02: 1769 windows created\n",
      "\n",
      "[3/5] Processing AP03\n",
      "--------------------------------------------------\n",
      "🔄 Processing AP03...\n",
      "   🏷️  Label distribution: {'Normal': 1616, 'Hypopnea': 150, 'Obstructive Apnea': 3}\n",
      "   ✅ AP02: 1769 windows created\n",
      "\n",
      "[3/5] Processing AP03\n",
      "--------------------------------------------------\n",
      "🔄 Processing AP03...\n",
      "   📊 Loaded signals: ['nasal_airflow', 'thoracic_movement', 'spo2']\n",
      "   🚨 Events: {'Hypopnea': 26, 'Obstructive Apnea': 2}\n",
      "   🪟 Extracted 1696 windows\n",
      "   📊 Loaded signals: ['nasal_airflow', 'thoracic_movement', 'spo2']\n",
      "   🚨 Events: {'Hypopnea': 26, 'Obstructive Apnea': 2}\n",
      "   🪟 Extracted 1696 windows\n",
      "   🏷️  Label distribution: {'Normal': 1679, 'Hypopnea': 16, 'Obstructive Apnea': 1}\n",
      "   ✅ AP03: 1696 windows created\n",
      "\n",
      "[4/5] Processing AP04\n",
      "--------------------------------------------------\n",
      "🔄 Processing AP04...\n",
      "   🏷️  Label distribution: {'Normal': 1679, 'Hypopnea': 16, 'Obstructive Apnea': 1}\n",
      "   ✅ AP03: 1696 windows created\n",
      "\n",
      "[4/5] Processing AP04\n",
      "--------------------------------------------------\n",
      "🔄 Processing AP04...\n",
      "   📊 Loaded signals: ['nasal_airflow', 'thoracic_movement', 'spo2']\n",
      "   🚨 Events: {'Hypopnea': 228, 'Obstructive Apnea': 9}\n",
      "   🪟 Extracted 1932 windows\n",
      "   📊 Loaded signals: ['nasal_airflow', 'thoracic_movement', 'spo2']\n",
      "   🚨 Events: {'Hypopnea': 228, 'Obstructive Apnea': 9}\n",
      "   🪟 Extracted 1932 windows\n",
      "   🏷️  Label distribution: {'Normal': 1765, 'Hypopnea': 166, 'Obstructive Apnea': 1}\n",
      "   ✅ AP04: 1932 windows created\n",
      "\n",
      "[5/5] Processing AP05\n",
      "--------------------------------------------------\n",
      "🔄 Processing AP05...\n",
      "   🏷️  Label distribution: {'Normal': 1765, 'Hypopnea': 166, 'Obstructive Apnea': 1}\n",
      "   ✅ AP04: 1932 windows created\n",
      "\n",
      "[5/5] Processing AP05\n",
      "--------------------------------------------------\n",
      "🔄 Processing AP05...\n",
      "   📊 Loaded signals: ['nasal_airflow', 'thoracic_movement', 'spo2']\n",
      "   🚨 Events: {'Hypopnea': 177, 'Obstructive Apnea': 142}\n",
      "   🪟 Extracted 1581 windows\n",
      "   📊 Loaded signals: ['nasal_airflow', 'thoracic_movement', 'spo2']\n",
      "   🚨 Events: {'Hypopnea': 177, 'Obstructive Apnea': 142}\n",
      "   🪟 Extracted 1581 windows\n",
      "   🏷️  Label distribution: {'Normal': 1256, 'Hypopnea': 182, 'Obstructive Apnea': 143}\n",
      "   ✅ AP05: 1581 windows created\n",
      "\n",
      "======================================================================\n",
      "📋 DATASET CREATION SUMMARY\n",
      "======================================================================\n",
      "✅ Total windows created: 8,800\n",
      "📊 Overall label distribution:\n",
      "   • Normal: 8,043 (91.4%)\n",
      "   • Hypopnea: 593 (6.7%)\n",
      "   • Obstructive Apnea: 164 (1.9%)\n",
      "\n",
      "💾 Saving dataset to: ../Dataset/\n",
      "💾 Saving dataset in multiple formats...\n",
      "   🏷️  Label distribution: {'Normal': 1256, 'Hypopnea': 182, 'Obstructive Apnea': 143}\n",
      "   ✅ AP05: 1581 windows created\n",
      "\n",
      "======================================================================\n",
      "📋 DATASET CREATION SUMMARY\n",
      "======================================================================\n",
      "✅ Total windows created: 8,800\n",
      "📊 Overall label distribution:\n",
      "   • Normal: 8,043 (91.4%)\n",
      "   • Hypopnea: 593 (6.7%)\n",
      "   • Obstructive Apnea: 164 (1.9%)\n",
      "\n",
      "💾 Saving dataset to: ../Dataset/\n",
      "💾 Saving dataset in multiple formats...\n",
      "   ✅ Features saved as Parquet: ../Dataset/breathing_dataset_features.parquet\n",
      "   ✅ Features saved as CSV: ../Dataset/breathing_dataset_features.csv\n",
      "   ✅ Features saved as Parquet: ../Dataset/breathing_dataset_features.parquet\n",
      "   ✅ Features saved as CSV: ../Dataset/breathing_dataset_features.csv\n",
      "   ✅ Raw time series saved as Pickle: ../Dataset/breathing_dataset_raw.pkl\n",
      "📊 Dataset formats saved:\n",
      "   • Parquet: 568,233 bytes (recommended for ML)\n",
      "   • CSV: 1,900,458 bytes (human-readable)\n",
      "   • Pickle: 145,282,894 bytes (full time series)\n",
      "📈 Statistics saved to: ../Dataset/dataset_stats.json\n",
      "======================================================================\n",
      "\n",
      "🎉 SUCCESS! Dataset created successfully!\n",
      "✅ Total windows: 8,800\n",
      "👥 Participants: 5\n",
      "📊 Label distribution: {'Normal': 8043, 'Hypopnea': 593, 'Obstructive Apnea': 164}\n",
      "📁 Files saved to: ../Dataset/\n",
      "\n",
      "📄 Files created:\n",
      "   • breathing_dataset_features.csv: 1,900,458 bytes\n",
      "   • breathing_dataset_features.parquet: 568,233 bytes\n",
      "   • breathing_dataset_raw.pkl: 145,282,894 bytes\n",
      "   • dataset_stats.json: 1,456 bytes\n",
      "   ✅ Raw time series saved as Pickle: ../Dataset/breathing_dataset_raw.pkl\n",
      "📊 Dataset formats saved:\n",
      "   • Parquet: 568,233 bytes (recommended for ML)\n",
      "   • CSV: 1,900,458 bytes (human-readable)\n",
      "   • Pickle: 145,282,894 bytes (full time series)\n",
      "📈 Statistics saved to: ../Dataset/dataset_stats.json\n",
      "======================================================================\n",
      "\n",
      "🎉 SUCCESS! Dataset created successfully!\n",
      "✅ Total windows: 8,800\n",
      "👥 Participants: 5\n",
      "📊 Label distribution: {'Normal': 8043, 'Hypopnea': 593, 'Obstructive Apnea': 164}\n",
      "📁 Files saved to: ../Dataset/\n",
      "\n",
      "📄 Files created:\n",
      "   • breathing_dataset_features.csv: 1,900,458 bytes\n",
      "   • breathing_dataset_features.parquet: 568,233 bytes\n",
      "   • breathing_dataset_raw.pkl: 145,282,894 bytes\n",
      "   • dataset_stats.json: 1,456 bytes\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE DATASET CREATION\n",
    "print(\"🚀 CREATING DATASET FROM SLEEP STUDY DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create dataset creator instance\n",
    "    creator = DatasetCreator(window_duration=30, overlap_ratio=0.5, sampling_rate=32)\n",
    "    \n",
    "    # Set input and output directories\n",
    "    input_dir = \"../Data\"\n",
    "    output_dir = \"../Dataset\"\n",
    "    \n",
    "    print(f\"📥 Input directory: {input_dir}\")\n",
    "    print(f\"📤 Output directory: {output_dir}\")\n",
    "    \n",
    "    # Create the dataset\n",
    "    print(\"\\n🔄 Starting dataset creation...\")\n",
    "    stats = creator.create_dataset(input_dir, output_dir)\n",
    "    \n",
    "    if stats and stats.get('total_windows', 0) > 0:\n",
    "        print(f\"\\n🎉 SUCCESS! Dataset created successfully!\")\n",
    "        print(f\"✅ Total windows: {stats['total_windows']:,}\")\n",
    "        print(f\"👥 Participants: {stats['participants']}\")\n",
    "        print(f\"📊 Label distribution: {stats['overall_distribution']}\")\n",
    "        print(f\"📁 Files saved to: {output_dir}/\")\n",
    "        \n",
    "        # Show files created\n",
    "        import os\n",
    "        if os.path.exists(output_dir):\n",
    "            files = [f for f in os.listdir(output_dir) if f.endswith(('.parquet', '.csv', '.pkl', '.json'))]\n",
    "            print(f\"\\n📄 Files created:\")\n",
    "            for file in sorted(files):\n",
    "                file_path = os.path.join(output_dir, file)\n",
    "                if os.path.exists(file_path):\n",
    "                    size = os.path.getsize(file_path)\n",
    "                    print(f\"   • {file}: {size:,} bytes\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Dataset creation failed - no windows were created\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during dataset creation: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
